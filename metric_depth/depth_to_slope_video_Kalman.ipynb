{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# METRIC DEPTH TO SLOPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "METRIC DEPTH ESTIMATION BY DEPTH ANYTHING PRETRAINED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import open3d as o3d\n",
    "from tqdm import tqdm\n",
    "from zoedepth.models.builder import build_model\n",
    "from zoedepth.utils.config import get_config\n",
    "import cv2\n",
    "from model import LFD_RoadSeg, UNet\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import time\n",
    "\n",
    "from math import pi,atan2,sqrt\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "# Global settings\n",
    "FL = 715.0873\n",
    "FY = 256 * 0.6\n",
    "FX = 256 * 0.6\n",
    "NYU_DATA = False\n",
    "FINAL_HEIGHT = 392*2\n",
    "FINAL_WIDTH = 512*2\n",
    "INPUT_DIR = './my_test/input'\n",
    "OUTPUT_DIR = './my_test/output'\n",
    "DATASET = 'kitti' # Lets not pick a fight with the model's dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kalman Filter Code for Result revision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = 1\n",
    "H = 1\n",
    "Q = 0\n",
    "R = 4\n",
    "\n",
    "def kalman_filter(z_meas, x_esti, P):\n",
    "    \"\"\"Kalman Filter Algorithm for One Variable.\"\"\"\n",
    "    # (1) Prediction.\n",
    "    x_pred = A * x_esti\n",
    "    P_pred = A * P * A + Q\n",
    "\n",
    "    # (2) Kalman Gain.\n",
    "    K = P_pred * H / (H * P_pred * H + R)\n",
    "\n",
    "    # (3) Estimation.\n",
    "    x_esti = x_pred + K * (z_meas - H * x_pred)\n",
    "\n",
    "    # (4) Error Covariance.\n",
    "    P = P_pred - K * H * P_pred\n",
    "\n",
    "    return x_esti, P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slope Estimation by using Camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Parameters of Camera\n",
    "\n",
    "h = 0.141 # camera heigts\n",
    "Y_c = 0.141 # camera heigts\n",
    "f = 390 # Focal Length\n",
    "\n",
    "\n",
    "h = 0.6\n",
    "Y_c = 0.6\n",
    "f = 1000\n",
    "#f = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function of Convolution for Finding Slope's Edge\n",
    "d = np.array([[0,1,0],[0,0,0],[0,-1,0]])\n",
    "def conv(input_data, filters = d, stride = 1, pad = 1):\n",
    "    H,W = input_data.shape\n",
    "    f_H,f_W = filters.shape\n",
    "    output_H = ( H + 2 * pad - f_H) // stride + 1\n",
    "    output_W = ( W + 2 * pad - f_W) // stride + 1\n",
    "\n",
    "    pad_data = np.pad(input_data, [(pad,pad),(pad,pad)],'constant')\n",
    "\n",
    "    output = np.zeros((output_H,output_W))\n",
    "\n",
    "    for h in range(output_H):\n",
    "        h_start = h * stride\n",
    "        h_end = h_start + f_H\n",
    "        for w in range(output_W):\n",
    "            w_start = w * stride\n",
    "            w_end = w_start + f_W\n",
    "            output[h,w] = np.sum(pad_data[h_start:h_end, w_start:w_end] * filters)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function of getting the lane center\n",
    "def getCenter(road):\n",
    "    center = []\n",
    "    up = 0\n",
    "    for i in range(100,390):\n",
    "        arr = np.where(road[i,:] == 1)\n",
    "        \n",
    "        if len(arr[0]) == 0:\n",
    "            pass\n",
    "            #print(arr)\n",
    "        else:\n",
    "            center.append([i,int(np.mean(arr[0]))])\n",
    "            \n",
    "    return center\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function of Getting Slope from Depth Map using Geometric method\n",
    "\n",
    "def getSlopeWithRoad(data,road):\n",
    "    \n",
    "    H,W = data.shape\n",
    "\n",
    "    # Get higher slope edge by using road segmentation\n",
    "    realRoad = data*road\n",
    "    midArr = getCenter(road)\n",
    "    for h,w in midArr:\n",
    "        h2,w2 = h,w\n",
    "        if 150 < w2 < 400:\n",
    "            break\n",
    "    \n",
    "    # Get gradient for getting lower slope edge\n",
    "    gradx,grady = np.gradient(realRoad)\n",
    "    # Get lower slope edge\n",
    "    slopeEdge1 = np.where(grady[h2+40:,w2] == np.max(grady[h2+40:H-10,w2]))\n",
    "    h1 = slopeEdge1[0][-1] + h2 + 40 + 15\n",
    "    if h1 > 390:\n",
    "        h1 = 390\n",
    "    w1 = 262\n",
    "\n",
    "    # Get the Slope of camera image\n",
    "    v_b = -(int(h2)- 392//2)\n",
    "    Z_tot = np.mean(data[h2,w2-10:w2+10])\n",
    "    Z_b = Z_tot - np.mean(data[h1,w1-10:w1+10])\n",
    "    tanb = (v_b*Z_tot/f + Y_c)/Z_b\n",
    "    theta = atan2(tanb,1)*180/pi\n",
    "    \n",
    "    return theta, h2, h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_tonumpy = lambda x : x.to('cpu').detach().numpy().transpose(0,2,3,1) # device 위에 올라간 텐서를 detach 한 뒤 numpy로 변환\n",
    "fn_denorm = lambda x, mean, std : (x * std) + mean\n",
    "fn_classifier = lambda x :  1.0 * (x > 0.5)  # threshold 0.5 기준으로 indicator function으로 classifier 구현\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# 변환 (Transform) 설정\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # 여기에 필요한 추가 변환을 넣으세요\n",
    "])\n",
    "\n",
    "def process_images(model,net):\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache() # GPU 캐시 데이터 삭\n",
    "    \n",
    "        try:\n",
    "            # Load file\n",
    "            filename = './my_test/input/501015deg.avi'\n",
    "            #filename = './my_test/input/10degree.avi'\n",
    "            raw_video = cv2.VideoCapture(filename)\n",
    "            filename = os.path.basename(filename)\n",
    "            \n",
    "                \n",
    "            # 동영상 저장용 코드\n",
    "            file_path = './my_test/output/result.mp4'\n",
    "            fps = 25.40\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'DIVX')            # 인코딩 포맷 문자\n",
    "            width = 518*2\n",
    "            height = 392*2\n",
    "            size = (int(width), int (height))                   # 프레임 크기\n",
    "            out = cv2.VideoWriter(file_path, fourcc, fps, size)\n",
    "\n",
    "            # Kalman Filter \n",
    "            # Initialization for system model.\n",
    "            global A,H,Q,R,x,P\n",
    "            A = 1\n",
    "            H = 1\n",
    "            Q = 0\n",
    "            R = 4\n",
    "            # Initialization for estimation.\n",
    "            x_0 = 0\n",
    "            P_0 = 6\n",
    "            x,P = x_0, P_0\n",
    "            \n",
    "            prev_time = 0\n",
    "\n",
    "            \n",
    "            while raw_video.isOpened():\n",
    "\n",
    "                    # get frame\n",
    "                    ret, frame = raw_video.read()\n",
    "                    frame_road_result = cv2.resize(frame, (518, 392))\n",
    "                    current_time = time.time() - prev_time\n",
    "\n",
    "                    if not ret :\n",
    "                        break\n",
    "                    if ret and current_time > 1/30:\n",
    "                        prev_time = time.time()\n",
    "                        #cv2.imshow('image',frame_road)\n",
    "                        if cv2.waitKey(30) == 27:\n",
    "                            break\n",
    "                    \n",
    "                    if current_time >= 1/29:\n",
    "                        continue            \n",
    "                \n",
    "\n",
    "                    # road segmentation\n",
    "                    frame_road = cv2.resize(frame, (224,224))\n",
    "                    frame_road = cv2.cvtColor(frame_road, cv2.COLOR_BGR2RGB)\n",
    "                    frame_road = transform(frame_road).unsqueeze(0)  # 배치 차원 추가\n",
    "                    frame_road = torch.tensor(frame_road).to(device)\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        output_road = net(frame_road)\n",
    "                    output_road = fn_tonumpy(fn_classifier(output_road))\n",
    "                    output_road = output_road.squeeze()\n",
    "                    \n",
    "                    # 0,1 reverse\n",
    "                    road = np.zeros_like(output_road)\n",
    "                    road[output_road == 0] = 1\n",
    "\n",
    "                    # visualize of roadseg\n",
    "                    road_img = (road * 255).astype(np.uint8)\n",
    "                    output_road_img = Image.fromarray(road_img)\n",
    "\n",
    "                    background = Image.fromarray(frame).resize((224,224)).convert('RGBA')\n",
    "                    output_road_img = output_road_img.convert('RGBA')\n",
    "                    output_road_img = Image.blend(background, output_road_img, alpha=0.5)\n",
    "                    numpy_image = np.array(output_road_img)\n",
    "                    opencv_image = cv2.cvtColor(numpy_image, cv2.COLOR_RGB2BGR)\n",
    "                    opencv_image = cv2.resize(opencv_image,(518,392))\n",
    "                    \n",
    "                    # equalizing the resolution of depth map\n",
    "                    road = Image.fromarray(road).resize((518,392))\n",
    "                    road = np.array(road)\n",
    "\n",
    "                    # Depth Estimation\n",
    "                    image_tensor = transforms.ToTensor()(frame).unsqueeze(0).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "                    pred = model(image_tensor, dataset=DATASET)\n",
    "                    \n",
    "                    if isinstance(pred, dict):\n",
    "                        pred = pred.get('metric_depth', pred.get('out'))\n",
    "                    elif isinstance(pred, (list, tuple)):\n",
    "                        pred = pred[-1]\n",
    "                    \n",
    "                    pred = pred.squeeze().detach().cpu().numpy()\n",
    "\n",
    "                    # Visualize the depth map\n",
    "                    depth = pred\n",
    "                    depth = (depth - depth.min()) / (depth.max() - depth.min()) * 255.0\n",
    "                    depth = depth.astype(np.uint8)\n",
    "                    depth = cv2.applyColorMap(depth, cv2.COLORMAP_INFERNO)\n",
    "                    \n",
    "                    # Calculate the slope\n",
    "                    theta, h2, h1= getSlopeWithRoad(pred,road)\n",
    "                    \n",
    "                    # Using Kalman Filiter\n",
    "                    \n",
    "                    # Initailize when the slope is ended\n",
    "                    if -0.5 < theta < 0.5:\n",
    "                        # Initialization for system model.\n",
    "                        A = 1\n",
    "                        H = 1\n",
    "                        Q = 0\n",
    "                        R = 4\n",
    "                        # Initialization for estimation.\n",
    "                        x_0 = 0\n",
    "                        P_0 = 6\n",
    "                        x,P = x_0, P_0\n",
    "\n",
    "                    x,P = kalman_filter(theta,x,P)\n",
    "                    x = round(x, 3)\n",
    "                    theta = round(theta, 3)\n",
    "\n",
    "                    # Display the Result\n",
    "                    #text = \"The slope is \" + str(x)\n",
    "                    text = \"Theta \" + str(theta) + \" Kalman \" + str(x)\n",
    "\n",
    "                    org=(100,100)\n",
    "                    result = np.full(shape=(392,518,3),fill_value=255,dtype=np.uint8)\n",
    "                    font=cv2.FONT_HERSHEY_SIMPLEX\n",
    "                    cv2.putText(result, text, org, font, 1,(0,0,0),2)\n",
    "\n",
    "                    # Draw line of slope edge\n",
    "                    cv2.line(frame_road_result, (10, h2), (500, h2),(255,0,0))\n",
    "                    cv2.line(frame_road_result, (10, h1), (500, h1),(0,255,0))\n",
    "                    \n",
    "                    resultShow = np.hstack((frame_road_result,opencv_image))\n",
    "                    resultShow2 = np.hstack((depth,result))\n",
    "                    result_final = np.concatenate((resultShow,resultShow2), axis=0)\n",
    "\n",
    "                    # Store the result \n",
    "                    out.write(result_final)\n",
    "\n",
    "                    # Show the result\n",
    "                    cv2.imshow('Result',result_final)\n",
    "                    if cv2.waitKey(30) == 27:\n",
    "                        break\n",
    "                    \n",
    "            out.release()\n",
    "            raw_video.release()\n",
    "            cv2.destroyAllWindows()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "        return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - 2024-05-13 00:07:38,516 - attention - xFormers not available\n",
      "WARNING - 2024-05-13 00:07:38,518 - block - xFormers not available\n",
      "INFO - 2024-05-13 00:07:38,582 - vision_transformer - using MLP layer as FFN\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params passed to Resize transform:\n",
      "\twidth:  518\n",
      "\theight:  392\n",
      "\tresize_target:  True\n",
      "\tkeep_aspect_ratio:  False\n",
      "\tensure_multiple_of:  14\n",
      "\tresize_method:  minimal\n",
      "Using pretrained resource local::./checkpoints/depth_anything_metric_depth_outdoor.pt\n",
      "Loaded successfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ZoeDepth(\n",
       "  (core): DepthAnythingCore(\n",
       "    (core): DPT_DINOv2(\n",
       "      (pretrained): DinoVisionTransformer(\n",
       "        (patch_embed): PatchEmbed(\n",
       "          (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
       "          (norm): Identity()\n",
       "        )\n",
       "        (blocks): ModuleList(\n",
       "          (0-23): 24 x NestedTensorBlock(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): MemEffAttention(\n",
       "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls1): LayerScale()\n",
       "            (drop_path1): Identity()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ls2): LayerScale()\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (head): Identity()\n",
       "      )\n",
       "      (depth_head): DPTHead(\n",
       "        (projects): ModuleList(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (2-3): 2 x Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (resize_layers): ModuleList(\n",
       "          (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))\n",
       "          (1): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (2): Identity()\n",
       "          (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "        (scratch): Module(\n",
       "          (layer1_rn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (layer2_rn): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (layer3_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (layer4_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (refinenet1): FeatureFusionBlock(\n",
       "            (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (resConfUnit1): ResidualConvUnit(\n",
       "              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (skip_add): FloatFunctional(\n",
       "                (activation_post_process): Identity()\n",
       "              )\n",
       "            )\n",
       "            (resConfUnit2): ResidualConvUnit(\n",
       "              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (skip_add): FloatFunctional(\n",
       "                (activation_post_process): Identity()\n",
       "              )\n",
       "            )\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (refinenet2): FeatureFusionBlock(\n",
       "            (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (resConfUnit1): ResidualConvUnit(\n",
       "              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (skip_add): FloatFunctional(\n",
       "                (activation_post_process): Identity()\n",
       "              )\n",
       "            )\n",
       "            (resConfUnit2): ResidualConvUnit(\n",
       "              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (skip_add): FloatFunctional(\n",
       "                (activation_post_process): Identity()\n",
       "              )\n",
       "            )\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (refinenet3): FeatureFusionBlock(\n",
       "            (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (resConfUnit1): ResidualConvUnit(\n",
       "              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (skip_add): FloatFunctional(\n",
       "                (activation_post_process): Identity()\n",
       "              )\n",
       "            )\n",
       "            (resConfUnit2): ResidualConvUnit(\n",
       "              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (skip_add): FloatFunctional(\n",
       "                (activation_post_process): Identity()\n",
       "              )\n",
       "            )\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (refinenet4): FeatureFusionBlock(\n",
       "            (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (resConfUnit1): ResidualConvUnit(\n",
       "              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (skip_add): FloatFunctional(\n",
       "                (activation_post_process): Identity()\n",
       "              )\n",
       "            )\n",
       "            (resConfUnit2): ResidualConvUnit(\n",
       "              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (skip_add): FloatFunctional(\n",
       "                (activation_post_process): Identity()\n",
       "              )\n",
       "            )\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (output_conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (output_conv2): Sequential(\n",
       "            (0): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (3): ReLU(inplace=True)\n",
       "            (4): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (seed_bin_regressor): SeedBinRegressorUnnormed(\n",
       "    (_net): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (3): Softplus(beta=1, threshold=20)\n",
       "    )\n",
       "  )\n",
       "  (seed_projector): Projector(\n",
       "    (_net): Sequential(\n",
       "      (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (projectors): ModuleList(\n",
       "    (0-3): 4 x Projector(\n",
       "      (_net): Sequential(\n",
       "        (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (attractors): ModuleList(\n",
       "    (0): AttractorLayerUnnormed(\n",
       "      (_net): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (3): Softplus(beta=1, threshold=20)\n",
       "      )\n",
       "    )\n",
       "    (1): AttractorLayerUnnormed(\n",
       "      (_net): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (3): Softplus(beta=1, threshold=20)\n",
       "      )\n",
       "    )\n",
       "    (2): AttractorLayerUnnormed(\n",
       "      (_net): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(128, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (3): Softplus(beta=1, threshold=20)\n",
       "      )\n",
       "    )\n",
       "    (3): AttractorLayerUnnormed(\n",
       "      (_net): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (3): Softplus(beta=1, threshold=20)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conditional_log_binomial): ConditionalLogBinomial(\n",
       "    (log_binomial_transform): LogBinomial()\n",
       "    (mlp): Sequential(\n",
       "      (0): Conv2d(161, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Conv2d(80, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (3): Softplus(beta=1, threshold=20)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For metric depth, using zoedepth model\n",
    "model_name = 'zoedepth'\n",
    "pretrained_resource = 'local::./checkpoints/depth_anything_metric_depth_outdoor.pt'\n",
    "config = get_config(model_name, \"eval\", DATASET)\n",
    "config.pretrained_resource = pretrained_resource\n",
    "model = build_model(config).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 불러오기\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "ckpt_dir = './checkpoint_lfd'  # 모델 파일 경로\n",
    "net = UNet().to(device)  # 모델 인스턴스 생성\n",
    "optim = torch.optim.Adam(net.parameters(), lr = 1e-3 )\n",
    "\n",
    "# 네트워크 불러오기\n",
    "def load(ckpt_dir,net,optim):\n",
    "    if not os.path.exists(ckpt_dir): # 저장된 네트워크가 없다면 인풋을 그대로 반환\n",
    "        epoch = 0\n",
    "        return net, optim, epoch\n",
    "\n",
    "    ckpt_lst = os.listdir(ckpt_dir) # ckpt_dir 아래 있는 모든 파일 리스트를 받아온다\n",
    "    #print(ckpt_lst)\n",
    "    ckpt_lst.sort(key = lambda f : int(''.join(filter(str.isdigit,f)))) # filter(str.isdigit, f) : f 에서 숫자만 뽑음 -> 체크포인트 모델을 epoch 순서대로 sort해서 최신꺼를 인덱싱 할 수 있게 한다.\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        dict_model = torch.load('%s/%s' % (ckpt_dir,ckpt_lst[-1])) # 저장된 모델 체크 포인트 중 가장 최신꺼를 가져온다\n",
    "        #print(dict_model)\n",
    "    else:\n",
    "      device = torch.device('cpu')\n",
    "      dict_model = torch.load('%s/%s' % (ckpt_dir,ckpt_lst[0]),map_location=device)\n",
    "\n",
    "\n",
    "\n",
    "    net.load_state_dict(dict_model['net'])\n",
    "    optim.load_state_dict(dict_model['optim'])\n",
    "    epoch = int(ckpt_lst[-1].split('epoch')[1].split('.pth')[0])\n",
    "\n",
    "    return net,optim,epoch\n",
    "\n",
    "net, optim, start_epoch = load(ckpt_dir = ckpt_dir, net = net, optim = optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "/tmp/ipykernel_27815/2271635973.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  frame_road = torch.tensor(frame_road).to(device)\n"
     ]
    }
   ],
   "source": [
    "process_images(model,net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
